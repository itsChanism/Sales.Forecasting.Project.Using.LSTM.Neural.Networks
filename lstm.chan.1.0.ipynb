{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa34639",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/__init__.py:36\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:26\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40022a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#######with leading 0s removed . the none removal version is in the 3rd cell\n",
    "# Function to reshape and scale the data\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Function to train the LSTM model and make forecasts\n",
    "def forecast_next_values_lstm(data_series, look_back=1):\n",
    "    # Normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(data_series.reshape(-1, 1))\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "\n",
    "    # Reshape into X=t and Y=t+1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    # Create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    trainPredict = model.predict(testX)\n",
    "    #testPredict = model.predict(testX)\n",
    "\n",
    "    # Invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY.reshape(-1, 1))\n",
    "    #testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "    # Plot the time series for each row\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #plt.plot(np.concatenate([trainPredict.ravel(), testPredict.ravel()]), label='Predicted', color='red')\n",
    "    plt.plot(np.concatenate([trainY.ravel(),trainPredict.ravel()]), label='Predicted', color='red')\n",
    "    plt.plot(np.concatenate([trainY.ravel(), testY.ravel()]), label='Actual', color='blue')\n",
    "\n",
    "    plt.title(f\"Row Index: {df.iloc[i, 0]}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the RMSE for the testing data\n",
    "    rmse = math.sqrt(mean_squared_error(testY, trainPredict))\n",
    "    print(f\"Testing RMSE for Row Index {df.iloc[i, 0]}: {rmse}\")\n",
    "    \n",
    "    error = rmse/np.std(data_series)\n",
    "    print(error)\n",
    "    return data_series,error\n",
    "\n",
    "df = pd.read_csv('cleansalesdata.csv')\n",
    "\n",
    "# Read the data starting from the fifth column\n",
    "df_clean = df.iloc[:, 5:]\n",
    "\n",
    "# Extract each row values and store it into a list\n",
    "list_per_row = [row.tolist() for row in df_clean.values]\n",
    "\n",
    "# Calculate the percentage to consider for the sum (10%)\n",
    "percentage_to_sum = 0.1\n",
    "\n",
    "# Find the number of elements to consider as the last 10%\n",
    "num_elements_to_sum = int(len(list_per_row[0]) * percentage_to_sum)\n",
    "\n",
    "# Filter out lists whose last 10% elements sum to 0\n",
    "list_per_row_filtered = [row for row in list_per_row if sum(row[-num_elements_to_sum:]) != 0]\n",
    "def remove_leading_zeros(rows):\n",
    "    new_rows = []\n",
    "    for row in rows:\n",
    "        while len(row) > 0 and row[0] == 0:\n",
    "            row = row[1:]\n",
    "        new_rows.append(row)\n",
    "    return new_rows\n",
    "\n",
    "# Remove leading zeros from each row\n",
    "list_per_row_filtered_no_zeros = remove_leading_zeros(list_per_row_filtered)\n",
    "\n",
    "# Iterate over each row of the filtered data\n",
    "for i, row in enumerate(list_per_row_filtered_no_zeros):\n",
    "    # Forecast the next values and get the error\n",
    "    _, error = forecast_next_values_lstm(np.array(row))\n",
    "    \n",
    "    # Append error, row index, and the original data to the lists\n",
    "    errors.append(error)\n",
    "    row_indexes.append(df.iloc[i, 0])\n",
    "    original_data.append(row)\n",
    "\n",
    "# Create a new DataFrame to store errors, row indexes, and original data\n",
    "error_df = pd.DataFrame({'Row Index': row_indexes, 'Normalized RMSE': errors, 'Original Data': original_data})\n",
    "\n",
    "# Export the DataFrame to a new CSV file\n",
    "error_df.to_csv('errors1.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfba72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####without leading 0s reomved\n",
    "\n",
    "# Function to reshape and scale the data\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Function to train the LSTM model and make forecasts\n",
    "def forecast_next_values_lstm(data_series, look_back=1):\n",
    "    # Normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(data_series.reshape(-1, 1))\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "\n",
    "    # Reshape into X=t and Y=t+1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    # Create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    trainPredict = model.predict(testX)\n",
    "    #testPredict = model.predict(testX)\n",
    "\n",
    "    # Invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform(trainY.reshape(-1, 1))\n",
    "    #testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "    # Plot the time series for each row\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    #plt.plot(np.concatenate([trainPredict.ravel(), testPredict.ravel()]), label='Predicted', color='red')\n",
    "    plt.plot(np.concatenate([trainY.ravel(),trainPredict.ravel()]), label='Predicted', color='red')\n",
    "    plt.plot(np.concatenate([trainY.ravel(), testY.ravel()]), label='Actual', color='blue')\n",
    "    plt.title(f\"Row Index: {df.iloc[i, 0]}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the RMSE for the testing data\n",
    "    rmse = math.sqrt(mean_squared_error(testY, trainPredict))\n",
    "    print(f\"Testing RMSE for Row Index {df.iloc[i, 0]}: {rmse}\")\n",
    "    \n",
    "    error = rmse/np.std(data_series)\n",
    "    print(error)\n",
    "    return data_series,error\n",
    "\n",
    "df = pd.read_csv('cleansalesdata.csv')\n",
    "\n",
    "# Read the data starting from the fifth column\n",
    "df_clean = df.iloc[:, 5:]\n",
    "\n",
    "# Extract each row values and store it into a list\n",
    "list_per_row = [row.tolist() for row in df_clean.values]\n",
    "\n",
    "# Calculate the percentage to consider for the sum (10%)\n",
    "percentage_to_sum = 0.1\n",
    "\n",
    "# Find the number of elements to consider as the last 10%\n",
    "num_elements_to_sum = int(len(list_per_row[0]) * percentage_to_sum)\n",
    "\n",
    "# Filter out lists whose last 10% elements sum to 0\n",
    "list_per_row_filtered = [row for row in list_per_row if sum(row[-num_elements_to_sum:]) != 0]\n",
    "\n",
    "\n",
    "# Create lists to store errors, row indexes, and original data\n",
    "errors = []\n",
    "row_indexes = []\n",
    "original_data = []\n",
    "\n",
    "# Iterate over each row of the filtered data\n",
    "for i, row in enumerate(list_per_row_filtered):\n",
    "    # Forecast the next values and get the error\n",
    "    _, error = forecast_next_values_lstm(np.array(row))\n",
    "    \n",
    "    # Append error, row index, and the original data to the lists\n",
    "    errors.append(error)\n",
    "    row_indexes.append(df.iloc[i, 0])\n",
    "    original_data.append(row)\n",
    "\n",
    "# Create a new DataFrame to store errors, row indexes, and original data\n",
    "error_df = pd.DataFrame({'Row Index': row_indexes, 'Normalized RMSE': errors, 'Original Data': original_data})\n",
    "\n",
    "# Export the DataFrame to a new CSV file\n",
    "error_df.to_csv('errors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59bf42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
